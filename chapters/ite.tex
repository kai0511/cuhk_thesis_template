\chapter{Evaluating Individualized Treatment Effects (ITE) of Risk Factors on Patient Outcomes}
\chaptermark{estimating individualized treatment effects}
\label{chap:ite}

\section{Motivation}
\label{sec:ite_mot}
  Traditional biomedical or clinical studies in the area of estimating treatment effect mainly focus on the average effect of risk factors (RFs) or treatment (tx) in population level. However, in the clinical environment we can easily find that the same risk factor may affect patients differently. Thus, patients may pay more attention to how a risk factor will affect them in an individual level rather than in a population level, given their clinical backgrounds and genetic characteristics. The main aim of this study is to resolve this concern by estimating the ITEs for each patient, with consideration of their unique genetic and clinical information. In this study we treat the two terms "risk factor" and "treatment" conceptually equivalent, since a risk factor can be considered as a "treatment" with adverse effects. The approach for estimating the ITEs for each patient allows us to offer tailored management to individual patients. This enables us to deliver more cost-effective prevention or treatment strategies to benefit them the most. This idea is also in the line with "personalized medicine", which has been advocated in recent years.

  In spite of an increasing number of studies in this area, current studies in ITE especially in biomedical research are rather limited. Some critical limitations include a lack of well-established validation methods for treatment effect estimations and the contribution of key features in ITE estimation, and failure in incorporating censored data. Even though genetic factors may determine heterogeneous response to tx/RFs, especially to cancer treatments \cite{fisher2013cancer}, current studies on ITE have not included genomic features. Here we proposed several methodologies to address the above limitations and applied the ITE framework to genomic data. In our approach genomic features were considered as risk factors or covariates that contribute to the heterogeneity of treatment effects.

\section{Background}
\label{sec:ite_bg}
  It has been well-known that different individuals response differently to the same risk factor (RF) or treatment (tx). For example, even though obesity is a risk factor for cardiometabolic (CM) diseases, there still are obese subjects who don't develop related complications \cite{neeland2018cardiovascular}. The type and severity of such CM complications can also show heterogeneity among subjects \cite{neeland2018cardiovascular}. Another evidence is that not all people suffering stressful life events are affected by depression, even if stressful events are risk factors for depression \cite{yang2015effects}. This fact can also be applied to other RFs or treatments. The heterogeneous effect can be contributed by different genetic and/or environmental factors of subjects, and these factors affect them differently. Here we would like to investigate the different treatment effect contributed by variants or mutations instead of clinical factors, since studies have shown that same variant/mutation can have varying outcomes on different subjects \cite{aggarwal2018influence, o2018cystic,vanderlaan2017mutations}.

  There are dramatic advances in omics technology and a sharp rising availability of biomedical data. However, current studies in cancer still mainly focus on one clinical/genetic RF at a time, without the consideration of presence of complex interactions among the subject's genetic and/or clinical factors. 
  
  One of most crucial concerns to patients is how a RF or treatment will affect them given their genetic and clinical information. However, current researches on this issue largely focus on the average treatment effect of RFs in population rather than individualized treatment effects.

  Here we built a computational framework to unravel the individualized effects of RFs/treatment so that we can estimate the treatment effect for each individual with the incorporation of his/her genetic and/or clinical background. We also developed methods to discover genetic and/or clinical features contributing the most to the estimation. We employed our approaches to cancer data to estimate treatment effects of genetic changes (e.g. changes of expression level of risk genes, mutations, CNVs etc.) and other RFs on each individual's survival.

\section{Overview of Related Work}
\label{sec:ite_overview}
  \subsection{Background Methods}
    Here we define notations for the clarification of following presentation. Let $\bX^{n \times m}$ denotes the covariate variable, $\bY^n$ denotes outcome variable, and $\bW^n$ denotes treatment variable. Given an observation $i$ we denote its covariates as $\mathbf{x_i}^m$, the risk factor/tx status as $w_i$ and outcome $y_i$. Here we restate that since a risk factor may also be considered as a "treatment" with adverse effects, methods for ITE estimation can also be employed to RFs. Assume that the outcome $\bY$ satisfy
    \begin{equation}
      \bY = f (\bX, \bW) + \mathbf{\epsilon},
    \end{equation}
    Where $\mathbf{\epsilon}$ follows $N(\mathbf{0}, \bSigma)$. $\bSigma$ is the covariance matrix.

    \textbf{Assumption} assume $\bX, \bW, \bY$ fulfill unconfoundedness assumption (randomization conditional on the covariates), 
    \begin{equation}
      \label{eqn:uncfd}
      \big[ \bY_i(1), \bY_i(0) \big] \indep \bW_i \given \bX_i. 
    \end{equation}
    
    Under the unconfoundedness assumption \ref{eqn:uncfd} the key is to estimate the expected difference, the estimation of ITE, for each individual in response between treatment and control. The ITE for subject $i$ is formulated as 
    \begin{equation}
      \tau(\mathbf{x_i}) = \mu_1 (\mathbf{x_i}) - \mu_0 (\mathbf{x_i}),
    \end{equation}
    with $\mu_1(x_i)$ and $\mu_0(x_i)$ defined as 
    \begin{equation}
      \label{eqn:ite_tau}
      \begin{split}
        \mu_1(x_i) & = \bbE(\bY = y_i|\bX=\mathbf{x_i}, \bW = w_i) = f(x_i, 1)\\
        \mu_0(x_i) & = \bbE(\bY = y_i|\bX=\mathbf{x_i}, \bW = 1 - w_i) = f(x_i, 0)
      \end{split}
    \end{equation} 
    respectively, where $w_i = 1$ without the loss of generality. For a given subject $i$, $y_i$ and $w_i$ are scalars, and $\mathbf{x_i}$ is a vector of length $m$. 
    Traditional machine learning method cannot handle this situation since they cannot capture the difference of ITE when the outcomes for RF/tx were absent or present. 

    A traditional solution to measure ITE is to estimate the difference of averaged outcome between treatments and controls in pre-specified subgroups \cite{gail1985testing} or subgroup defined by learning algorithms \cite{su2009subgroup, su2011interaction, athey2016recursive,foster2011subgroup}. Su et. al. employed interaction tree to iteratively searching subgroups based on treatment effect \cite{su2009subgroup,su2011interaction}. Similarly, causal trees proposed by Athey and Imbens estimate the treatment effect at the leaves of the tree \cite{athey2016recursive}. However, main drawbacks of the approach are that there is no ground truth for subgroup definition, and that the impediment of iteratively searching for subgroups present obvious treatment effect and reporting only the results for subgroups with extreme treatment effects to highlight heterogeneity may be highly spurious \cite{assmann2000subgroup,cook2004subgroup}. In the high dimensional setting, it's still very challenging to divide subjects into appropriate subgroups \cite{powers2017some}. It's the same case for genomic data. 
    
    Alternatively, a feasible approach is to use any supervised machine learning (SML) methods to fit $\mu_1(\mathbf{x})$ and $\mu_0(\mathbf{x})$ simultaneously / separately and estimate the difference by putting them together. Specifically, one may fit a single model $\mu(\mathbf{x},w)$ or separate models for the treated and control groups, and compute the different between $\mu(\mathbf{x},w)$ and $\mu(\mathbf{x}, 1-w)$. Studies \cite{lu2018estimating,dasgupta2014risk} utilize different counterfactual random forests algorithms to estimates treatment effects by fitting separate random forests models to treatment and control groups. Several literature, including Green and Kern \cite{green2012modeling}, Hill \cite{hill2011bayesian}, and Hill and Su \cite{hill2013assessing}, has employed bayesian forest-based machine learning methods to estimate heterogeneous treatment effects. These studies utilize Bayesian additive regression tree (BART) method \cite{chipman2010bart}, and can obtain reliable intervals for treatment effects by MCMC sampling. For lasso-like methods for causal inference \cite{imai2013estimating,tian2014simple}, it's difficult to capture interactions, which may be naturally present in genomic data, in high-dimensional setting, in split of its simplicity and good ability in feature selection. A limitation of these studies is lack of formal statistical inference results \cite{wager2018estimation}. Some other methods, like Meta learners and deep learning based, for ITE estimation could be found in \cite{kunzel2019metalearners, johansson2016learning}.

    Here we are interested in methods with following characteristics: automatically select important features, well capture high interactions present, and have good asymptotic properties. Thus, methods such as causal forests \cite{wager2018estimation} or GRF \cite{athey2019generalized} are much more preferred. Causal forests \cite{wager2018estimation} have been proposed with an objective to maximize the heterogeneity of $\tau(\mathbf{x})$. Recently, Athey et al. proposed an extension of causal forests, GRF \cite{athey2019generalized}, inspired by the R-learner proposed in \cite{nie2017quasi}. Both of the two methods \cite{wager2018estimation,athey2019generalized} inherits the excellent capability of random forest in capturing complex interactions. 
    
    However, there are still substantial research gaps, including relative lack of methods for result validation, evaluation of key features contributing to ITE estimation and handling censored data. Here we proposed methods to address these key issues and pioneer new applications to genomic data, which is the first of its kind.

  \subsection{Causal Forests}
  \label{sec:ite_cf}
    In this section, we will explain causal forests (CF) technically, a basis of our ITE framework. CF \cite{wager2018estimation} originate from random forests \cite{breiman2001random}, which are related to kernel  or nearest neighborhood methods. However, random forests differ in that they determine weights received by nearby observations in a data-driven way, and this characteristics is critical in high dimensional environment or the present of high order interactions among covariates \cite{wager2018estimation}. This is the same case for CF. 
    
    Here we begin with causal trees (CT) \cite{athey2016recursive} since CF are made of a number of CT. In this part we follow a similar notations as appeared in \cite{athey2016recursive}. A tree can be considered as a partitioning of the feature space $\mathbb{X}$, denoting as $\Pi$. A partition $\Pi$ with a number of elements $\#(\Pi)$ can be written as 
    \begin{equation*}
      \Pi = \{ l_1, l_2, \dots, l_{\#(\Pi)} \}, 
    \end{equation*}
    and a union of all elements in partition is the whole feature space $\bbX$.

    Let $\bbP$ denote the space of partitions, and $\bbS$ be the space of samples from a population of observations. We seek for a algorithm $\pi: \bbS \rightarrow \bbP$ that splits sample space $\bbS$ into partition $\Pi$. 

    Given a partition $\Pi$ and sample $\mathrm{S}$, the estimated conditional mean for observation $\bfx$ is 
    \begin{equation*}
      \hat{\mu} (\bfx; \Pi) \equiv \frac{1}{\#(i \in \mathrm{S}: \bfx_i \in l(\bfx; \Pi))} \sum_{i \in \mathrm{S}: \bfx_i \in l(\bfx; \Pi)} y_i, 
    \end{equation*}
    which is an unbiased estimator for $\mu (\bfx; \Pi)$. Here $l(\bfx; \Pi)$ is the leaf to which $\bfx$ belongs.

    A adjusted MSE criteria including $\bbE[\bY_i^2]$, a term that does not depend on the estimator, is defined as 
    \begin{equation}
      \label{eqn:ite_mse}
      \mathrm{MSE}_\mu (\mathrm{S^{te}}, \mathrm{S^{est}}, \Pi) \equiv \frac{1}{\# (\mathrm{S^{te}})} \sum_{i 
      \in \mathrm{S^{te}}} \bigg \{ (y_i - \hat{\mu}(\bfx_i; \mathrm{S^{est}}, \Pi) )^2  - y_i^2 \bigg \}.
    \end{equation}

    The expectation of the modified MSE is 
    \begin{equation*}
      \mathrm{EMSE}_\mu (\Pi) \equiv \bbE_{\mathrm{S^{te}}, \mathrm{S^{est}}} \big [ \mathrm{MSE}_\mu (\mathrm{S^{te}}, \mathrm{S^{est}}, \Pi) \big ].
    \end{equation*}

    The objective of CT is to maximize the criteria
    \begin{equation}
      Q^\mathrm{H}(\pi) \equiv - \bbE_{\mathrm{S^{te}}, \mathrm{S^{est}}, \mathrm{S^{tr}}} \big [ \mathrm{MSE}_\mu (\mathrm{S^{te}}, \mathrm{S^{est}}, \pi(\mathrm{S^{tr}})) \big ].
    \end{equation}
    This criteria shows better convergence properties of confidence intervals, compared with conventional practice that $\mathrm{S^{est}}$ and $\mathrm{S^{tr}}$ are the same sample for both tree construction and estimation \cite{wager2018estimation}.

    With the above setup for treatment effect estimation, a similar definition to \ref{eqn:ite_mse}, is defined as
    \begin{equation}
      \label{eqn:ite_tau_mse}
      \mathrm{MSE}_\tau (\mathrm{S^{te}}, \mathrm{S^{est}}, \Pi) \equiv \frac{1}{\# (\mathrm{S^{te}})} \sum_{i 
      \in \mathrm{S^{te}}} \bigg \{ (\tau_i - \hat{\tau}(\bfx_i; \mathrm{S^{est}}, \Pi) )^2  - \tau_i^2 \bigg \}.
    \end{equation}
    In reality, $\tau_i$ cannot be directly observed. The estimated counterparts are defined as 
    \begin{equation}
        \hat{\tau}(\bfx; \mathrm{S}, \Pi) \equiv \hat{\mu}(1, \bfx, \mathrm{S}, \Pi) - \hat{\mu}(1, \bfx; \mathrm{S}, \Pi),
    \end{equation}
    where 
    \begin{equation*}
      \hat{\mu}(w, \bfx, \mathrm{S}, \Pi) \equiv \frac{1}{\#(i \in \mathrm{S_w}: \bfx_i \in l(\bfx; \Pi))} \sum_{i \in \mathrm{S_w}: \bfx_i \in l(\bfx; \Pi)} y_i^{\mathrm{obs}}.
    \end{equation*}
    
    With the fact that $\hat{\tau}$ is constant within each leaf and the fact that 
    \begin{equation*}
      \bbE_{\mathrm{S^{te}}} [ \tau_i | i \in \mathrm{S^{te}}: i \in l(\bfx, \Pi) ] = \bbE_{\mathrm{S^{te}}} [ \hat{\tau} (\bfx; \mathrm{S^{te}}, \Pi)],
    \end{equation*}
    
    A crucial estimator for the infeasible in-sample goodness-of-fit criterion is derived as 
    \begin{equation}
      - \mathrm{MSE}_\tau (\mathrm{S^{tr}}, \mathrm{S^{tr}}, \Pi) = \frac{1}{N^{\mathrm{tr}}} \sum_{i \in \mathrm{S^{tr}}} \hat{\tau}^2 (\bfx_i; \mathrm{S^{tr}}, \Pi).
    \end{equation}
    Then this leads to an estimator for the criterion relying only on $\mathrm{S^{tr}}$ and $N^{\mathrm{est}}$
    \begin{equation}
      \label{eqn:ite_honest_tau}
      \begin{split}
        - \hat{\mathrm{EMSE}}_\tau (\mathrm{S^{tr}}, N^{\mathrm{est}}, \Pi) \equiv & \frac{1}{N^{\mathrm{tr}}} \sum_{i \in \mathrm{S^{tr}}} \hat{\tau}^2 (\bfx_i; \mathrm{S^{tr}}, \Pi) - \\
          & \big( \frac{1}{N^{\mathrm{tr}}} + \frac{1}{N^{\mathrm{est}}} \big) \sum_{l \in \Pi} \bigg(  \frac{\mathrm{S}_{\mathrm{S_{tx}^{tr}}}^2 (l)}{p} + \frac{\mathrm{S}_{\mathrm{S_{ctl}^{tr}}}^2 (l)}{1-p} \bigg)
      \end{split}
    \end{equation}
    where the last term in the second line of \ref{eqn:ite_honest_tau} is pooled within-leaf variance. Definitely the splits of the tree are chosen to maximize the variance of $\tau(\bfx_i)$. For more detailed derivations, refer to original publication \cite{athey2016recursive}.

    Briefly, the objective for splitting for the adaptive version of CT, denoted CT-A, uses $-\mathrm{MSE}_\tau (\mathrm{S^{tr}}, \mathrm{S^{tr}}, \Pi)$. The same objective function also is applicable to CV version of CT-A but evaluated at the samples $\mathrm{S^{tr, cv}}$ and $\mathrm{S^{tr,cv}}$. The splitting objective function for the honest CTS, CT-H, is $-\mathrm{MSE}_\tau (\mathrm{S^{tr}}, N^{\mathrm{est}}, \Pi)$. The same objective function also can be applied to a CV version of CT-H, but evaluated at the cross-validation sample $\mathrm{S^{tr, cv}}$ with known $N^{\mathrm{est, cv}}$.

    Procedure for causal forests with honesty and subsampling is proposed as follows:
    \begin{algorithm}
      \caption{Causal forests with honesty and subsampling}
      \label{cf_algthm}
      \begin{algorithmic}[1]
        \REQUIRE a samples $\mathrm{S}^{n \times m}$ and pre-specified parameters, including the number of trees $\mathrm{B}$, \textit{mtry} and the sub-sampling rate $\mathrm{s}$ 
        \FORALL{$i$ such that $0\leq i\leq \mathrm{B}$}
          \STATE samples for model construction $\mathrm{S_i^{tr, est}} \leftarrow$ \uppercase{Subsample} (S, s), with remaining as test set $\mathrm{S_i^{te}}$
          \STATE training samples $\mathrm{S_i^{tr}, S_i^{est}} \leftarrow$ \uppercase{Subsample} ($\mathrm{S_i^{tr, est}}$)
          \STATE causal tree $\mathrm{T_i} \leftarrow \mathrm{CT} (\mathrm{S_i^{tr}, S_i^{est}})$
          \STATE make out-of-bag prediction $\hat{\tau}(\bfx_j^i)$ for $\bfx_j^i \in \mathrm{S_i^{te}}$
        \ENDFOR
        \RETURN CTs $\mathrm{T_1}, \mathrm{T_2}, \dots , \mathrm{T_i}, \dots, \mathrm{T_B}$ and $\hat{\tau}(\bfx_j)$ by averaging all available out-of-bag predictions $\tau_j^{(.)}$ for $\bfx_{j} \in S$.

      \end{algorithmic}
    \end{algorithm}
    In above table, $\mathrm{CT}$ stands for causal tree algorithm defined above in the section.

\section{ITE Framework}
\label{sec:ite_model}
  In order to assess the ITE of RFs on disease outcome and discover key features that contribute to estimation of the heterogeneity we proposed an analytic framework to estimate the ITE of RFs/tx, with genetic features as primary RFs and/or covariates. The term 'individualized treatment effect' (ITE) will be used regardless of an RF or treatment being considered, since we have declared that the two are conceptually equivalent entities in this study. 

  The estimated ITE may be formulated as \ref{eqn:ite_tau} under a counterfactual outcomes framework \cite{rubin2005causal}. In reality the true value of $\tau(\mathbf{x})$ cannot be directly observed, as we only have one of the two potential outcomes. In observational studies, the tx assignment may be associated with potential outcomes due to confounding variables. If the unconfoundedness assumption \ref{eqn:uncfd} satisfies then the causal ITE can still be captured. Then, in most observational studies, the study is still of significance in despite of the presence of residual confounding. 
  
  In that case we may still gain insights into TE heterogeneity at an association level, and covariates responsible for heterogeneity may still deserve further studies, and in practice a continuous variable $\bW$ is also allowed \cite{athey2019generalized}. When $\bW$ is continuous, an average partial effect is estimated $Cov [\bY,\bW \given \bX = \bfx] / Var[\bW \given \bX = \bfx]$, which may be considered as the increase in $Y$ given a unit increase in $W$, conditional on the covariates. 
  
  Our experiment studies rely on the framework of GRF, as it is a state-of-the-art approach which directly optimizes an objective function for ITE estimation, rather than fit conditional means for treatment and control observations. However, most of the methodologies and extensions presented in this study is data-driven such that it can be widely applied to any other ITE estimation models.

  \subsection{Novel Tests for the presence of heterogeneity}
  \label{sec:ite_novel_tests}
    Unlike a SML model, an ITE model is not straightforward since the actual TE is not directly observed. There is no either empirically or theoretically well-established methods in ITE validation. 
    We notice that the function \textR{test\textunderscore calibration} provided in the R package \textR{grf} \cite{tibshirani2018package}, and the idea behind it is borrowed from \cite{chernozhukov2018generic}. Thus,  we would compare our methods with it in simulations in future section \ref{sec:ite_res}.
    
    We would propose several novel statistical tests for ITE model evaluation:

    \textbf{Split-half correlation with multiple splits}  This method borrows the idea of cross validation (CV) in SML. Briefly, we proposed to split the dataset into 2 halves. Specifically, an ITE model is first fitted on the 1st half, then applied to the 2nd half to predict $\tau(\bfx)$. The process can be repeated by reversing the training and testing sets. Then for each half, we have out-of-bag predictions $\tau_\mathrm{oob}(\bfx)$ and predictions $\tau_\mathrm{pred}(\bfx)$ by applying models fitted on the other half to it. We can assess the model fitting by examining the correlation between the two $\tau(\bfx)$s.

    The rational behind this statistical test for the presence of heterogeneity is that we expect the ITE to vary randomly around a constant in the absence of heterogeneous treatment effects that can be explained by covariates, and hence the correlation between the two estimated $\tau$ values should be close to 0; otherwise, the stability and generality of ITE models can be examined by checking the replicability on an independent dataset using split-half correlation. 
    
    In order to reduce random variations that may be introduced by a single split, we performed split-half correlation test with multiple splits on the data in practice and combined the results together. Standard Simes test \cite{simes1986improved} may be an options for the combination, since it is robust to positive dependency of p-values. Its alternative hypothesis $(\mathrm{H1})$ is that at least one of the hypotheses is non-null (at least one out of n splits yields a positive significant correlation), so it may be relatively loose for our problem. To increase stringency, we introduced a partial conjunction test (partial Simes test) based on the work from Benjamini et al. \cite{benjamini2008screening}, whose $\mathrm{H1}$ assumes that at least $r$ out of the $n$ splits yield a positive significant correlation. 
    
    The threshold $r$ defines the stringency/level of consistency for a finding that deserves further study. In experiment we set $r$ to be 10\% of $n$, which can give adequate type I error control, and employed 3 correlation measures (Pearson, Spearman and Kendall) to evaluate the split-half correlation.
  
    \textbf{A new permutation framework} We proposed a novel permutation statistical test to assess the presence of heterogeneity. That is, it can be used to assess whether the predicted ITE are significantly better than predictions assuming a constant TE (which is the norm in most studies). 

    The objective of CF is to maximize $\Var(\tau)$, as stated in section \ref{sec:ite_cf}. When there is no heterogeneity that can be explained by covariates, $\Var(\tau)$ should be low and close to 0. Equivalently, in this situation $\Var(\tau)$ is roughly equal to $\Var(\tau)$ yielded by model fitted on arbitrary covariates. Thus, a permutation approach we proposed is based on this rational to test the significance of $\Var(\tau)$ observed. 
    
    To model the null hypothesis we shuffled the covariates for each permutation, such that there is no heterogeneity that can be explained by covariates, and then computed the $\Var_\mathrm{observed}$ for the permuted data. 
    
    If we repeat this process $N$ times, then a probability for the null hypothesis of $\Var(\tau)$ statistical test is defined as 
    \begin{equation}
      \Pr (\mathrm{null_{var}}) = \frac{\#(\Var_\mathrm{perm}(\hat{\tau})\geq \Var_\mathrm{observed}(\hat{\tau})}{N},
    \end{equation}
    
    A related but clinically relevant question is: does the model that allows ITE outperform the 'standard' model predicting a constant ITE? That is, whether patients benefit more with the introduction of individualized treatments than a conventional treatments with a consideration of averaged treatment effects only. In ordinary regression problem, the goodness-of-fit of model is assessed by the mean squared error (MSE) between the expected outcome and predictions, so it's preferred to compute the mean squared error (MSE) between the true and estimated $\tau$ for model assessment. If  ITE model has a lower MSE than a constant model, which assumes that the ITE is the same for every subject, then ITE model outperforms the constant one. In reality, the true $\tau(x)$ cannot be directly observed, but Nie et al. \cite{nie2017quasi} proposed that the MSE between the true and estimated $\tau(\bfx)$, or $\tau_\mathrm{risk}(\mathrm{\bfx})$ can be defined as 
    \begin{equation}
      \begin{split}
        \hat{\tau}_\mathrm{risk} & = \sum_{i} \bigg ( (y_i - \hat{y}(\bfx_i) - \big (w_i - \hat{w}(\bfx_i) \big) \hat{\tau}(\bfx_i) \bigg ) ^2 \\
        & = \sum_{i} \bigg( \tilde{y_i} - \tilde{w_i} \hat{\tau}(\bfx_i) \bigg )^2.
      \end{split}
    \end{equation}

    Here $\hat{y}(\bfx_i)$ is an estimate of $\bbE(y_i|\bX=\bfx_i)$ and $\hat{w}(\bfx_i)$ is an estimate of $\Pr(w_i=1 \given \bX=\bfx_i)$ by any SML models. For simplicity, $\tilde{y_i}$ stands for $y_i - \hat{y}(\bfx_i)$, and $\tilde{w}_i (\bfx_i)$ for $w_i - \hat{w}(\bfx_i)$. The two terms can be regarded as residualized outcome and treatment respectively. These quantities are out-of-bag estimations. We can then compute the $\tau_\mathrm{risk}$ assuming an unrestricted $\hat{\tau}(\bfx)$ estimated from an ITE model and a constant effect based on the \textbf{average treatment effect (ATE)} $\bar{\tau}(\bfx)$. We proposed the following definition to assess the improvement in $\tau_\mathrm{risk}$ due to the incorporation of ITE versus a constant treatment effect
    \begin{equation}
      \hat{\tau}_\mathrm{improve} = \sum_{i}(\tilde{y} - \tilde{w} \hat{\tau(x_i)})^2 - \sum_{i}(\tilde{y} - \tilde{w} \bar{\tau(x_i)})^2.
    \end{equation}

    To model the null distribution of $\hat{\tau}_\mathrm{improve}$, we propose a permutation approach in which permuted $\hat{\tau}_\mathrm{improve}$s are obtained by shuffling the covariates for a predefined number of times. The null hypothesis of above test is that there is no statistical difference between the observed $\hat{\tau}_\mathrm{improve}$ and permuted $\hat{\tau}_\mathrm{improve}$s. Note that the test does not require any distributional assumptions.
    
    An adaptive permutation strategy with early stopping was adopted to reduce the computing time. Permutations will be stopped earlier if the result is unlikely to be significant in future runs. Specifically, we will calculate a $99\%$ CI for the permutation p-value after each $k (k \ll N)$ runs. Here $N$ is the total number permutations. If the lower CI $> 0.05$, the permutation will be terminated early.

  \subsection{Modeling Survival Outcomes}
    Time-to-event data are common in biomedical research, and standard ITE estimation methods may not work on survival data due to censoring or lost to follow-up. Usually some subjects have not experienced the event at the end of follow-up, that is, their records of survival time are unavailable (right censored), so the actual survival time for them is unknown. We proposed a flexible approach that can incorporate survival data into GRF and any other ITE models, an approach based on weighted 'mean imputation'.

    Given a subject, denote its actual survival time as $T_i$ and its censor time as $c_i$. In reality we observe $y_i$ which is $\min(T_i, c_i)$ due to censoring. Let $t_{(1)}< t_{(2)} < \dots < t_{(j)}$ be the censored survival times in ascending order, and $\hat{K}$ be the Kaplan-Meier (KP) estimator function of survival. Given $T_i>c_i$ for subject $i$, its log of censored survival times can be estimated by 
    \begin{equation}
      \log (y_i^{*}) = \sum_{t(j) > T_i^c} \log t(j) \frac{\Delta \hat{T}(t(j))} {\hat{T}(T_i^c)},
    \end{equation}
    where $\Delta \hat{T}(t(j))$ refers to to the jump size of  $\hat{K}$ at $t_{(j)}$ \cite{datta2007predicting}. Under the assumption of the log-normal distribution of survival time, the imputed survival times can be included in ITE estimation. 

\section{Experiment Results}
\label{sec:ite_res}

  \subsection{Simulations Studies}
    In design of simulations to evaluate the performance of our ITE framework and to compare the power and type I error rate of our proposed statistical tests with that of $\textR{test\_calibration}$ provided in the R package \textR{grf}, we adopted similar strategies in the generation of synthetic data as introduced in \cite{powers2017some}. Here we assume the log survival time is normally distributed without loss of generality. We considered the following six elements in simulations design:
    \begin{enumerate}
      \item \textit{Sample size} The number of observations in the data is $n$, and $p$ is the number of covariates.
      \item \textit{Distributions of covariates} Across all simulation scenarios, we drawn samples from standard normal distribution for features with odd column number, and for features with even column number we sampled from a Bernoulli distribution with $p = 1/2$. For simplicity, we denotes the distribution for the policy as $D_x$.
      \item \textit{Key functions} we denote propensity function for observations receiving a treatment $\pi(\cdot)$, average treatment effect $\mu(\cdot)$, and treatment effect $\tau(\cdot)$. The conditional mean effect for treatments and controls can be designed to be $\mu_1(\cdot) = \mu(\cdot) + \tau(\cdot)/2 $ and $\mu_0(\cdot) = \mu(\cdot) - \tau(\cdot)/2 $ respectively.
      \item \textit{Generation of survival time} Under the log-normal distribution of survival time, the survival time of observation can be generated by taking the natural exponentiation of the mean effect using $\exp(\cdot)$.
      \item \textit{Censor} We considered a censor rate of roughly 20\% for all our scenarios. Given the uncensored simulated survival time, we found the cutoff $r$ for the 80\% quantile, and then generated censor time $T(\cdot)$ using the exponential distribution with rate parameter $1/r$. If the simulated $\log Y_i > T(\bfx_i)$, then $T(\bfx_i)$ should be used as outcome; otherwise $\log Y_i$ was used.
      \item \textit{Noise levels} The noise level $\sigma_{\log(Y)}^2$ was introduced in the generation of log survival time $\log Y_i$, which sampled from a normal distribution with mean $\mu(\cdot) + (w - 1/2) \tau(\cdot)$ and variance $\sigma_{\log(Y)}^2$, where $w \sim \mathrm{Bernoulli}(\pi(\cdot))$. 
    \end{enumerate}  
    Given the above predefined components, our data generation for  observations $i$ is modeled as
    \begin{equation*}
      \begin{split}
        \bfx_i & \sim D_x, \\
        W_i & \sim \mathrm{Bernoulli}\big( \pi(\bfx_i) \big), \\
        \log Y_i & \sim \mathrm{Normal} \big( \mu(\bfx_i) + (W_i - 1/2) \tau(\bfx_i), \sigma_{\log(Y)}^2  \big), \\
        T_i & \sim \Exp(1/r),\\
        c_i & = \begin{cases} 1, \hspace{0.2cm} \text{if } \log Y_i \leq T_i \\ 0, \hspace{0.2cm} \text{otherwise } \end{cases}, \\
        Y_i & = \exp( \min(\log Y_i, T_i) ),
      \end{split}
    \end{equation*}
    where $r$ is a cutoff corresponding to a specific quantile of $\log Y_i$, and $\pi(\bfx_i)$ is defined as
    \begin{equation}
      \label{eqn:ite_pi}
      \pi(\bfx_i) = \frac{\exp(\mu(\bfx_i) - \tau(\bfx)/2)}{1 + \exp(\mu(\bfx_i) - \tau(\bfx)/2)} 
    \end{equation}
    for observational studies; for randomized studies $\pi(\bfx_i) = 1/2$ for all $\bfx_i$, which is the same as defined in \cite{powers2017some}. In practice, the value for 0.8 quantile  was used. $c_i$ is an indicator for censor. If there is an event ($\log Y_i \leq T_i$) for subject $i$, then $c_i$ is 1; otherwise 0.
    
    We used functions with minor changes from \cite{powers2017some} for propensity probability of receiving a treatment $\pi(\cdot)$, average treatment effect $\mu(\cdot)$, and treatment effect $\tau(\cdot)$ . Within the simulation experiments, both randomized and observational studies are included, and 8 different functions of mean and treatment effects are made here to represent a combination of univariate/multivariate, additive/ interactive, and linear and piecewise constant relationships. They are defined as follows:
    \begin{equation*}
      \begin{split}
        f_1(x) & =  0, f_2(x) = 5 \bbI (x_1 > 1) - 5 * pnorm(-1),  f_3(x) = 5 x_1,\\
        f_4(x) & = x_2 x_4 x_6 + 2 x_2 x_4 (1 - x_6) + 3x_2 (1 - x_4) x_6 \\
        & + 4 x_2 (1 - x_4) (1 - x_6) + 5 (1 - x_2) x_4 x_6 + 6 (1 - x_2) x_4 (1 - x_6)\\
        & + 7 (1 - x_2) (1 - x_4) x_6 + 8 (1 - x_2) (1 - x_4) (1 - x_6) - 4.5, \\
        f_5(x) & = x_1 + x_3 + x_5 + x_7 + x_8 + x_9, \\
        f_6(x) & = 4 \bbI (x_1 > 1) \bbI (x_3 > 0) + 4 \bbI (x_5 > 1) \bbI (x_7 > 0) + 2 x_8 x_9 - 4 * pnorm(-1), \\
        f_7(x) & = \frac{1}{\sqrt{2}} (x_1^3 + x_2 + x_3^3 + x_4 + x_5^3 + x_6 + x_7^3 + x_8 + x_9^3 - 7) \\
        f_8(x) & = \frac{1}{2} (f_4(x) + f_5(x)),
      \end{split}
    \end{equation*}
    where $pnorm(x)$ is a function that calculates the cumulative distribution probability $F(x) = \Pr(X \leq x)$. Here $X$ is with standard normal distribution. 

    \begin{table}[htbp]
      \centering
      \caption{Specifications for simulation scenarios}
        \begin{tabular}{l|cccccccc}
        \toprule
        \multirow{2}[2]{*}{} & \multicolumn{8}{l}{\textbf{Scenarios}} \\
              & 1,9   & 2,10  & 3,11  & 4,12  & 5,13  & 6,14  & 7,15  & 8,16 \\
        \midrule
        $n$        & 300       & 300       & 200       & 600      & 400      & 300      & 450      & 700 \\
        $p$        & 400       & 400       & 300       & 300      & 200      & 200      & 100      & 100 \\
        $\mu(x)$   & $f_8(x)$  & $f_5(x)$  & $f_4(x)$  & $f_7(x)$ & $f_3(x)$ & $f_1(x)$ & $f_2(x)$ & $f_6(x)$ \\
        $\tau(x)$  & $f_1(x)$  & $f_2(x)$  & $f_3(x)$  & $f_4(x)$ & $f_5(x)$ & $f_6(x)$ & $f_3(x)$ & $f_8(x)$ \\
        $\sigma_{\log Y}^2 $ & 1   & 1/4   & 1         & 1/4      & 1        & 1        & 4        & 4 \\
        \bottomrule
        \end{tabular}%
      \label{tab:ite_scenarios}%
    \end{table}%

    The 8 functions listed are centered and scaled to have mean close to 0 and roughly the same variance. Table \ref{tab:ite_scenarios} gives the specifications for simulation scenarios, including sample size $n$, number of features $p$, functions for mean and treatment effect $\mu(\cdot)$ and $\tau(\cdot)$, and variance of noise $\sigma_{\log Y}^2$. Specifications for sample size have been adjusted to accommodate our simulated survival data, compared with those in study \cite{powers2017some}. Scenarios of odd number are randomized experiments, with $\pi(\bfx_i) = 1/2$ for all $\bfx_i$, but scenarios of even number are observational studies, in which $\pi(\bfx_i)$ is defined by equation \ref{eqn:ite_pi} for each subject $\bfx_i$. Note there is no heterogeneity in treatment effects in scenario 1 and 9.

    \begin{table}[htbp]
    \caption{Comparison of power/type I error rate of different tests for the presence of heterogeneity}
    \centering
      \begin{threeparttable}
          \begin{tabular}{c|cccccc}
          \toprule
          Scenarios & SHC-P & SHC-K & SHC-S & TC & $\Var_{\tau}$ & $\tau_{\mathrm{improve}}$ \\
          \midrule
          1*    & 0.002 & 0.002 & 0.002 & 0.002 & 0.058 & 0.054 \\
          2     & 0.266 & 0.268 & 0.26  & 0.896 & 0.976 & 0.98 \\
          3     & 1     & 1     & 1     & 1     & 1     & 1 \\
          4     & 0.656 & 0.652 & 0.652 & 0.782 & 0.924 & 0.924 \\
          5     & 0.548 & 0.548 & 0.546 & 0.752 & 0.926 & 0.934 \\
          6     & 0.888 & 0.88  & 0.882 & 0.944 & 0.992 & 0.992 \\
          7     & 0.862 & 0.844 & 0.85  & 0.886 & 0.956 & 0.974 \\
          8     & 0.752 & 0.732 & 0.73  & 0.42  & 0.592 & 0.664 \\
          9*    & 0.004 & 0.004 & 0.004 & 0.002 & 0.054 & 0.068 \\
          10    & 0.162 & 0.164 & 0.164 & 0.652 & 0.856 & 0.88 \\
          11    & 0.672 & 0.666 & 0.672 & 1     & 1     & 1 \\
          12    & 0.904 & 0.908 & 0.908 & 0.92  & 0.968 & 0.986 \\
          13    & 0.636 & 0.628 & 0.632 & 0.742 & 0.924 & 0.934 \\
          14    & 0.6   & 0.6   & 0.598 & 0.74  & 0.938 & 0.95 \\
          15    & 0.784 & 0.774 & 0.774 & 0.604 & 0.85  & 0.87 \\
          16    & 0.99  & 0.986 & 0.988 & 0.956 & 0.978 & 0.988 \\
          \bottomrule
          \end{tabular}%
          \begin{tablenotes}
            \small
            \item 1. SHC-P stands for split-half correlation with pearson method, SHC-K for split-half correlation with kendall method, and SHC-S for split-half correlation with kendall method method for correlation testing.
            \item 2. TC represents \textR{test\_calibration} from the R package \textR{GRF}.
            \item 3. $\Var_{\tau}$ and $\tau_{\mathrm{improve}}$ stand for two statistical tests for the presence of heterogeneity using them, proposed in section \ref{sec:ite_novel_tests}.
            \item 4. Scenario 1 and 9 are marked with stars, since they are two scenarios without heterogeneity.
          \end{tablenotes}
      \end{threeparttable}
    \label{Tab:ite_sim_res}%
    \end{table}%

    To explore the reliability of our developed statistical tests and compare them with the \textR{test\_calibration} (TC) from GRF, we repeated application of our ITE framework with different random seed to the above simulation scenarios 500 times and examined the fitting of our approach with the statistical tests and test\_calibration for each run. We calculated the proportion of repeats with p-values $\leq 0.05$ for every statistical test. Because of no heterogeneity in scenarios 1 and 9, the proportion of tests returning significant findings (\textit{p} <0.05)  in scenarios 1 and 9 represents the type I error rates. The same proportion represent the power of statistical methods for other simulation scenarios. Simulation results are shown in table \ref{Tab:ite_sim_res}.

    The statistical tests $\Var_{\tau}$ and $\tau_{\mathrm{improve}}$ and TC showed good validity in our simulations. They all had strong power in capturing the presence of heterogeneity, and notably our methods with $\Var_{\tau}$ and $\tau_{\mathrm{improve}}$ completely dominated TC provided in package GRF across all simulation scenarios except 1 and 9. In scenarios 1 and 9 they all showed relatively low type I error rate, with roughly 0.05 for statistical tests using $\Var_{\tau}$ and $\tau_{\mathrm{improve}}$ and 0.002 for TC. Type I error rates for our methods were also within an acceptable range, and TC had lower type I error rate than our methods. However, our methods with $\Var_{\tau}$ and $\tau_{\mathrm{improve}}$ were more powerful in detecting the presence of heterogeneity.

    Split-half correlation (SHC) approaches with three different correlation evaluation methods showed good type I error rates in scenario 1 and 9. However,  their powers varied greatly across all other scenarios. By investigating their performance in scenario 6, 7, 12, and 16, we found that they also can maintain good performance if there is strong heterogeneity. Surprisingly, they apparently outperformed the other three methods in Scenario 8. In short, we can still consider SHC approaches as good supplements to the other three methods.

  \subsection{Applications to Real Data}
    COVID-19 has become a major public health burden and the latest report by WHO showed that COVID-19 has infected over 15 million  people and caused more 600,000 deaths throughout the world \cite{dong2020interactive}. Most infected people experience mild to moderate symptoms and recover without special treatment, but a subgroup of patients develop severe complications, including acute respiratory distress syndrome (ARDS) \cite{ellinghaus2020genomewide}.  Many clinical risk factors may contribute to higher severity of the disease, such as diabetes and other chronic illnesses. Nevertheless, there is substantial heterogeneity of outcomes among people with risk factors.  Here we aim to identify clinical/genetic risk factors that demonstrate effect heterogeneity (on severity of illness). 

    \begin{table}[htbp]
      \centering
      \caption{Results for test of model fitting by SHC and permutation for selected clinical variables with genetic expression from blood}
      \begin{threeparttable}
        \begin{tabular}{lccccc}
        \toprule
        \multicolumn{1}{c}{\multirow{2}[0]{*}{\textbf{Clinical variables}}} & \multicolumn{5}{c}{\textbf{P-values}} \\
              & \textbf{Pearson} & \textbf{Kendall } & \textbf{Spearman } & \textbf{Perm var} & \textbf{Perm risk} \\
        \midrule
        Alcohol intake frequency & 1.111E-05 & 8.412E-03 & 9.016E-03 & \textbf{0.076} & \textbf{0.024} \\
        Diabetes diagnosed & 5.985E-32 & 6.818E-12 & 6.203E-12 & \textbf{0.000} & \textbf{0.070} \\
        Alcohol drinker status & 5.680E-04 & 3.154E-03 & 3.489E-03 & \textbf{0.000} & 0.966 \\
        Age at recruitment & 1.236E-05 & 6.283E-04 & 6.292E-04 & \textbf{0.076} & \textbf{0.070} \\
        Cholesterol & 9.323E-09 & 1.585E-04 & 1.823E-04 & \textbf{0.010} & 0.472 \\
        HbA1c & 1.131E-05 & 2.361E-02 & 2.238E-02 & \textbf{0.008} & 0.402 \\
        Ethnic background & 3.672E-08 & 2.468E-04 & 1.813E-04 & \textbf{0.000} & 0.144 \\
        LDL direct & 1.201E-05 & 5.942E-03 & 5.890E-03 & \textbf{0.094} & 0.678 \\
        \bottomrule
        \end{tabular}%
        \begin{tablenotes}
          \small
          \item 1. Perm var stands for permutation variance test, and Perm risk for permutation $\tau$-risk test. 
          \item 2. P-values $< 0.1$ for Perm var and Perm risk are in bold.
        \end{tablenotes}
      \end{threeparttable}
      \label{tab:ite_blood_result}%
    \end{table}%

    We applied our ITE framework to GWAS data of patients with COVID-19. The data was extracted from UK Biobank, a large and long-term biobank study in the United Kingdom, which aims to investigate genetic and environmental determinants of diseases \cite{biobank2014uk}. We considered clinical factors shown to affect COVID-19 susceptibility in Atkins et al.\cite{atkins2020preexisting} as risk factors and covariates in our model. As effect heterogeneity of the risk factors may be contributed by the varied genetic background of subjects, we also considered gene expression profiles as covariates. Genetic expression of each subject was imputed by PrediXcan \cite{gamazon2015gene}. The imputation of gene expression by PrediXcan is tissue-specific; here we examined the imputed expression of blood and lung.
    
    We considered COVID-19 positive patients only.  The severity of disease was considered as the outcome; based on relatively limited data provided by the UK Biobank, we labeled 'severe' disease if a patient received inpatient treatment.  A total of 1550 patients were include, among which 1023 required hospitalization and were labelled as 'severe'. The rest (N=527) were not hospitalized and were assumed to be having a milder disease. Since there were missing values for some clinical variables in the data, we imputed missing data using missForest \cite{stekhoven2012missforest} with default settings. The program iteratively employs random forests to impute the missing values.

    In practice, we considered one clinical variable as treatment, severity as outcome, and all other clinical and genetic factors as covariates for each run. If there were at least two out of three p-values $< 0.05$ from the split-half correlation (SHC) test, then permutation tests would also be carried out to further validate the ITE model. Imputed expression for lung and blood were modeled separately with clinical variables. Results for blood and lung are summarized in Table \ref{tab:ite_blood_result} and \ref{tab:ite_lung_result} respectively. 

    \begin{table}[htbp]
      \centering
      \caption{Results for test of model fitting by SHC and permutation for selected clinical variables with genetic expression from lung}
      \begin{threeparttable}
        \begin{tabular}{lccccc}
        \toprule
        \multicolumn{1}{c}{\multirow{2}[0]{*}{\textbf{Clinical variables}}} & \multicolumn{5}{c}{\textbf{P-values}} \\
              & \textbf{Pearson} & \textbf{Kendall } & \textbf{Spearman } & \textbf{Perm var} & \textbf{Perm risk } \\
        \midrule
        Alcohol intake frequency & 4.639E-08 & 2.127E-04 & 1.420E-04 & 1.000 & \textbf{0.092} \\
        Diabetes (type 2) diagnosed  & 3.736E-54 & 1.575E-17 & 2.063E-17 & \textbf{0.000} & \textbf{0.000} \\
        Age HBP diagnosed & 5.351E-04 & 1.803E-02 & 1.757E-02 & 1.000 & 0.200 \\
        Alcohol drinker status & 3.331E-06 & 1.251E-02 & 1.317E-02 & \textbf{0.010} & \textbf{0.038} \\
        Age at recruitment & 4.973E-11 & 8.332E-05 & 1.005E-04 & 1.000 & 0.200 \\
        GPC   & 1.966E-02 & 4.935E-02 & 5.539E-02 & \textbf{0.050} & 0.296 \\
        Cholesterol & 1.298E-18 & 5.319E-06 & 5.150E-06 & 1.000 & 0.117 \\
        HbA1c & 3.778E-18 & 2.461E-07 & 1.778E-07 & \textbf{0.044} & \textbf{0.020} \\
        HDL cholesterol & 3.136E-04 & 2.598E-02 & 2.456E-02 & 1.000 & 0.400 \\
        Ethnic background & 1.651E-17 & 2.640E-08 & 1.776E-08 & \textbf{0.000} & 0.310 \\
        LDL direct & 4.337E-18 & 9.382E-05 & 8.628E-05 & 1.000 & \textbf{0.048} \\
        \bottomrule
        \end{tabular}%
      \end{threeparttable}
      \begin{tablenotes}
        \small
        \item 1. Perm var stands for permutation variance test, and Perm risk for permutation $\tau$-risk test. 
        \item 2. P-values $< 0.1$ for Perm var and Perm risk are in bold.
        \item 3. HBP stands for high blood pressure, and GPC for Genetic principal components.
      \end{tablenotes}
      \label{tab:ite_lung_result}%
    \end{table}%

    Clinically, HbA1c is a blood test that is used to diagnose and monitor glycemic control in patients with diabetes. Our results indicate that type 2 diabetes and HbA1c demonstrate effect heterogeneity on the severity of COVID-19 infection, with p-values of SHC  $<$ 5E-17 on lung data and $<$ 1E-11 on blood data and of permutation test $<$ 0.001 on lung data and $<$ 0.07 on blood data. The results indicate that the risk conferred by diabetes or high HbA1c is likely different for different individuals. Previous studies have suggested diabetes or poor glycemic  control as risk factors for COVID-19 or more severe disease \cite{mehra2020cardiovascular,fang2020patients,zhu2020association}, but our results also suggest that the risk conferred can differ across subjects with the same risk factor, which may have implications in treatment and prevention. Similarly, our results also reveal cholesterol, HDL-cholesterol and LDL-cholesterol are possible risk factors with effect heterogeneity. These clinical variables are known risk factors for cardiometabolic diseases, and studies have demonstrated that cardiometabolic comorbidities are related to a more severe course of COVID-19 \cite{stefan2020obesity,guo2020cardiovascular,shi2020association}.
    
    Interestingly, previous studies suggested that alcohol consumption is associated with the amount of ACE2 present in the body and particularly in lung \cite{okuno1986mild,testino2020patients}, which in turn may lead to increased susceptibility to infection. The present findings further suggested the effect of alcohol drinking may be heterogeneous across individuals. We note, however that the link between alcohol drinking and infection might be subject to confounding variables like sharing alcoholic drinks \cite{mungmungpuntipantip2020sharing} and propensity to social gathering \cite{andersen2020early}. Our study also shows ethnic background and age may be risk factors demonstrating effect heterogeneity. A recent study has shown that black ethnicity are at higher risk and South Asians and other ethnicities have intermediate risks compared with the white, and that people aged 80+ years are more likely to have severe infection \cite{atkins2020preexisting}. However, heterogeneity of the racial effects on people with different background is unknown. 

\section{Conclusion}
\label{sec:ite_conclusion}
  In the chapter, we proposed a computational framework to estimate individualized treatment effects (ITE) of risk factors on patient outcome. In order to assess model fitting, we propose three different criteria to investigate the model fitting. In order to improve the applicability of ITE framework across different scenarios, we proposed an approach based on weighted "mean imputation" to incorporate survival times as outcomes. We carried out simulations to validate our framework on survival outcomes. Our method showed good power and valid type I error control across different simulation settings. In real data analysis, we applied our approach to GWAS data of patients with COVID-19, and showed that certain risk factors (e.g. type 2 diabetes) is associated with heterogeneous effect across different subjects. 

  Our approach may have following limitations. In practice, we found that split-half correlation method has better power than permutation-based methods. We conjecture that this is due to the characteristics of GWAS data, in which each covariate has very small effect and lots of covariates contribute to the outcome. Further work will include a larger panel of simulations with different distribution of effects. The application to COVID-19 is a preliminary study although the results were interesting. Further work is required to further characterize the variables interacting with the risk factors, which may contribute to the heterogeneity. The current sample size is relatively small and power to detect ITE may not be sufficient. Hospitalization is a rough proxy for severity and more detailed clinical data will enable better delineation of outcomes. Finally, we expect some unknown confounders may be missed so the estimated effect cannot be considered as entirely a 'causal' effect. Potential extension also include modeling survival among infected patients in the UK Biobank data. 
\chapterend